
\section{Introduction}
\label{s:intro}

In this work, we successfully replicated the results of the article \textit{A Neurodynamical Model for Working Memory}\supercite{bib:NeurodynamicalModel} written by Razvan Pascanu and Herbert Jaeger.
This replication has been done using Python, and the code is available on \href{https://github.com/theoboraud/ESN}{GitHub}\supercite{bib:githubRepo}.
In their article, they propose (1) a way to implement working memory and (2) a way to characterise working memory states. \\
\\
(1) First, they implemented a working memory model in the form of a Recurrent Neural Network (RNN), more precisely an Echo State Network (ESN).
In their model, working memory corresponds to special output units trained to maintain information through feedback connections.
Their model had been trained to predict the next character in a sequence randomly generated depending on a context, and to maintain this context in working memory in order to help the prediction. We replicated the same model, trained it to solve mostly the same task and obtained comparable results. \\
\\
(2) They characterised the working memory states of their model by defining a notion of attractors for input driven dynamical systems.
In autonomous dynamical systems the memory states could have been characterised for instance by the stable fixed points of the dynamic.
However with inputs, these points are transformed into blobs stable against input that doesn't aim to change the memory state.
In our replication we also obtain qualitatively the same result.
However in the paper they go further and propose a method to automatically find the attractors of an input driven dynamical system.
They applied this method only to a toy model and not to the working memory model so we didn't implement that part. \\
\\
In \textbf{Section~\ref{s:method}} we give all the implementation details of the model and the tasks it aims to solve.
We also highlight the few details that were missing in the paper and that we had to make a choice on in order to reproduce the results.
Then, in \textbf{Section~\ref{s:result}}, we compare the results we obtain to their results in the former article, to see whether or not we had been able to successfully replicate the experiments.

\section{Methods}
\label{s:method}

\subsection{Model}
\label{s:model}

Their model is a slightly modified Echo State Network, a special kind of Recurrent Neural Network where only the readout weights are trained.
\textbf{Figure \ref{fig:architecture}} illustrates and sums up the overall architecture of the model.
It is composed of a recurrent internal layer of size $\mathbf{N}$, a.k.a. reservoir, $\mathbf{K}$ input, $\mathbf{L}$ normal output units that are not fed back to the reservoir, and $\mathbf{WM}$ additional special output units.
They call the latter the Working Memory units (or WM-units).
These WM-units have a recurrent trainable connection to other WM-units (including themselves), and a feedback connection to the reservoir units.

In the following we use the following notations to describe the dynamic of their model:
\begin{itemize}
    \item Input units activations $\mathbf{u}$, with weights matrix $\mathbf{W^{in}}$ for input connections;
    \item Internal units activations $\mathbf{x}$, with weights matrix $\mathbf{W}$ for internal connections;
    \item Output units activations $\mathbf{y}$, with weights matrix $\mathbf{W^{out}}$ for output connections;
    \item WM-units activations $\mathbf{m}$, with weights matrix $\mathbf{W^{mem}}$ for connections from the input units, the reservoir and the WM-units to the WM-units; 
    \item Weights matrix $\mathbf{W^{b}}$ for the feedback connections from the WM-units to the reservoir. 
\end{itemize}

Given these notations, the dynamic of their model is as follow: 
\begin{itemize}
    \item Eq. (\ref{eq:xn+1}) represents the dynamic of the internal units. This dynamic involves at the same time the state of the internal units given their previous state, the previous state of the WM-units and the actual state of input units. We use $\mathbf{}{f}$ to subsume its activation functions, which are the hyperbolic tangent in this model.
    \begin{equation} \label{eq:xn+1}
        x(n+1) = f(W^{in}u(n+1) + Wx(n) + W^{b}m(n))
    \end{equation}
    
    \item Eq. (\ref{eq:mn+1}) represents the dynamic of the WM-units. This dynamic involves the state of the WM-units given the actual state of input and internal units, and the actual state of the WM-units. The sharp threshold function $\mathbf{f^{m}}$ is the activation function for the WM-units, and is given by Eq. (\ref{eq:fm}), while $\mathbf{W^{mem}}$ will be computed by linear regression during the step 1 of the model \textit{Training} phase as shown in Eq. (\ref{eq:Wmem}).
    \begin{equation} \label{eq:mn+1}
        m(n+1) = f^{m}(W^{mem}(u(n+1), x(n+1)))
    \end{equation}
    
    \item Eq. (\ref{eq:yn+1}) represents the dynamic of the output units. This dynamic involves the state of the output units given the actual state of input and internal units. $\mathbf{f^{out}}$ is the activation function of the output units, and is the Identity function, while $\mathbf{W^{out}}$ will be computed by linear regression during the step 2 of the model \textit{Training} phase as shown in Eq. (\ref{eq:Wout}).
    \begin{equation} \label{eq:yn+1}
        y(n+1) = f^{out}(W^{out}(u(n+1), x(n+1)))
    \end{equation}
    
    \item Eq. (\ref{eq:fm}) is the sharp threshold function used as the activation function of the WM-units.
    \begin{equation} \label{eq:fm}
        f^{m} = \begin{cases}
            -0.5 \; \; x\leq 0. \\
            +0.5 \; \; x>0. \end{cases}
    \end{equation}
    
    \item Eq. (\ref{eq:Wmem}) is the equation to compute the weights matrix for WM-units using linear regression. The activations of the reservoir, the input and the target of the WM-units are all stored in matrix $\mathbf{H}$, $\mathbf{M_{target}}$ is the target of the WM-units, and $\dagger$ stands for the pseudo-inverse.
    \begin{equation} \label{eq:Wmem}
        W^{mem} = (H^{\dagger} \cdot M_{target})^T\\
    \end{equation}
    
    \item Eq. (\ref{eq:Wout}) is the equation to compute the weights matrix for output units using linear regression. The activations of the reservoir and the input units are all stored in matrix $\mathbf{G}$, $\mathbf{Y_{target}}$ is the target of the output units, and $\dagger$ stands as before for the pseudo-inverse.
    \begin{equation} \label{eq:Wout}
        W^{out} = (G^{\dagger} \cdot f^{out^{-1}}(Y_{target}))^T\\
    \end{equation}
    
\end{itemize}

When the model is started, the weights are initialized as stated:
\begin{itemize}
    \item We have $\mathbf{K}=13$, $\mathbf{N}=1200$, $\mathbf{L}=65$ and $\mathbf{WM}=6$.
    \item $\mathbf{W^{in}}$ is of size $\mathbf{N \times K}$, with 10\% of which are +0.5, 10\% are -0.5, and the rest is 0.
    \item $\mathbf{W}$ is of size $\mathbf{N \times N}$, with only 12000 random non-zeros connections, which will randomly either take the value +0.1540 or -0.1540.
    \item $\mathbf{W^{b}}$ is of size $\mathbf{N \times WM}$, where every weight is either +0.4 or -0.4.
    
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width = \linewidth]{data/Architecture.png}
    \caption{Model architecture. See \textbf{Section~\ref{s:model}} for further details.}
    \label{fig:architecture}
\end{figure}

\subsection{Input generation}

Pascanu and Jaeger also describe the method they used to generate the input of the model.
The input consists of a constant bias (-0.5) and a column of 12 pixels of an image.
Each column of 12 pixels of the image is given as input to the network one after the other.

This image represents a sequence of characters, i.e. curly brackets (opened or closed) or 65 other ASCII symbols (e.g. letters in lower cases, numbers...).
To each of these characters we associate a number that is its position in an alphabet sequence (see \textbf{Table~\ref{tab:alphabet}}).
For the sake of simplicity, in the following we say character $i$ for the $i$-th character in that sequence.
The sequence of characters is generated randomly using different 7 Markov Chains, and the Markov Chain used depends on the current curly bracket level.
More precisely, the next character is either:
\begin{itemize}
    \item an open (resp. closed) curly bracket thus increasing (resp. decreasing) the current bracket level by 1 to a maximum (resp. minimum) of 6 (resp. 0)
    \item or generated with a Markov chain given the last character that was not a curly bracket
\end{itemize}
Given that the next character is not a curly bracket, if the current bracket level is $\mathit{j}$, the next character after $\mathit{i}$ will either be:
\begin{itemize}
    \item $\mathit{i} + \mathit{j} + 1$ modulo 65 with probability $0.8$;
    \item any of the 64 other characters from the alphabet with probability $\frac{0.2}{64} = 0.003125
$.
\end{itemize}
During the training phase, the probability for the next character to be an open (resp. closed) curly bracket is 0.15.
Whereas in the testing phase it is 0.03, which forces the WM-unit to maintain the current bracket level for a longer time. \\
\\
The sequence is transformed into an image by displaying and concatenating each of its characters. 
The characters are displayed using a randomly selected font and then uniformly randomly stretched between 6, 7 or 8 pixels in width.
Finally a salt-and-pepper noise of amplitude 0.1 and probability 1 is added to the whole image.\\
\\
See \textbf{Table \ref{tab:sequence_example}} to observe an example of input sequence generated from the alphabet in \textbf{Table \ref{tab:alphabet}} in \textbf{Section \ref{s:input_gen2}}.

\subsection{Task}

The aim of the WM-units is to keep track of the current bracket level. When the current bracket level is $\mathit{k}$, the $\mathit{k}$ first WM-units should be at +0.5, whereas the remaining ones should be at -0.5. \\
\\
The aim of the output units is to predict the next character in the sequence when the current character is not a curly bracket.
The prediction for the next character should be made in the middle of receiving the current character. If character $i$ should be predicted, then all the output units but the $i$-th one should be set to 0 whereas the $i$-th one should be 1. \\

\begin{figure}[h]
    \centering
    \includegraphics[width = \linewidth]{data/Sequence_img.png}
    \caption{Input and target outputs of the network. The first row represents the image input sent to the network columns by columns; the second is the target values for WM-units, with each white rows representing the time steps on which the corresponding WM-units has a value of 0.5, and in black when its value is -0.5 (a.k.a. white when the corresponding bracket level has been reached, and black when not); the third one is the target output, with 65 black pixels (value 0) at the middle of the character representation, and the next predicted character represented by a white pixel (value 1) depending of its location in the alphabet (see \textbf{Table \ref{tab:alphabet}}).}
    \label{fig:archi tecture}
\end{figure}

\subsection{Training and testing}

As they did, we train $W^{mem}$ and $W^{out}$ separately with different input sequences.
First we train train $W^{mem}$ using teacher forcing and then we train $W^{out}$.
We use a sequence of 10000 characters to train $W^{mem}$, and of 49000 characters to train $W^{out}$.
The teacher signal for training $W^{mem}$ is defined at all time whereas for training $W^{out}$ it is defined only in the middle of characters that are not curly brackets.
Finally we test the model with a 35000 characters long sequence.
As an error on the bracket level would propagate and create more errors, each time the model makes a mistake on the bracket level in the WM-units we count it and correct it.
These errors are classified between \textit{false positives} (i.e. the network detects a bracket when there is none) and \textit{false negatives} (i.e. the network fails to detect a bracket).
The errors made by the output units are also counted but not corrected. The error rate of prediction is then computed given the number of characters the network failed to predict correctly and the total length of the sequence.

\subsection{Attractors}

Following the instructions in the original article, we run the WM model 7 times, each time with a different sequence with no bracket, and forcing the WM-unit to a fixed bracket level (\textit{k} = 0 for the first, \textit{k} = 1 for the second, etc...). Each sequence has a length of 6500 characters, for about 45,000 network updates in total. We collect in each case both the reservoir states and the input nits. Then, every sets of reservoir states (resp. input vectors) are concatenated, and their first principal components (or PCs) are computed. Finally, for each of the 7 original sets, the first PC of the inputs is plotted against the first two PCs of the reservoir states.

\subsection{Implementation choices}

In the original article very few parts were unclear. Pascanu and Jaeger briefly explain how they choose to generate the input, but did not specify the exact method and tools they used to implement it. This subsection aims at listing out the choice we made and the tools we used to obtain similar results than the original paper.

\subsubsection{Language and libraries}

Since the language used for the former model was not specified, we decided to use Python (3.7.4) to implement our model.
The article did not specify which tools they used to transform their symbolic sequence into an image.
We chose to use the libraries \href{https://pillow.readthedocs.io/en/stable/}{\textit{PIL}}, \href{https://github.com/rougier/freetype-py}{\textit{FreeType}} and \href{https://www.scipy.org/}{\textit{SciPy}} for this purpose.

\subsubsection{Input sequence alphabet}
\label{s:input_gen2}

In the original paper Pascanu and Jageger did not specify which exact characters they used for their 65 symbols, and therefore not the exact order of this alphabet. For this, we decided to create our own, as shown on \textbf{Table \ref{tab:alphabet}}. We can observe in \textbf{Table \ref{tab:sequence_example}} an input sequence example in order to further understand the behaviour of the input generation algorithm with our alphabet.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|}
        \hline
        \textbf{Alphabet sequence order} \\ \hline \hline
        abcdefghijklmnopqrstuvwxyz0123456789\ !"\#\$\%\&'()*+,.-\_/:;<=>?@€|$[]$§\\ \hline % abcdefghijklmnopqrstuvwxyz0123456789 !\"#$%&'()*+,.-_/:;<=>?@|€[]§
    \end{tabular}
    \caption{Order of alphabet for data generation.}
    \label{tab:alphabet}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|}
        \hline
        \textbf{Input sequence example} \\ \hline \hline
        abc\{eg\textcolor{red}{0}24\{7\{!\textcolor{red}{o}sw0\textcolor{red}{-}\}:=\{\{\textcolor{red}{f}kp\\ \hline
    \end{tabular}
    \caption{Example of input sequence. Each character in red corresponds to a randomly chosen character which does not depend on the current character and the memory states (with a probability of 20\%). Use \textbf{Table \ref{tab:alphabet}} to understand how the input is selected 80\% of the time depending on the bracket level (i.e. when the characters are in black).}
    \label{tab:sequence_example}
\end{table}

\subsubsection{Font}

We have not been able to find the fonts used in the original model (\textit{FreeMono} classic, oblique, bold and bold oblique of \href{https://gimp.software/2018/11/13/download-gimp-2-3-6/}{Gimp 2.3.6}).
Therefore, we tried in the first place to use these \href{https://www.fontspace.com/gnu-freefont/freemono}{\textit{FreeMono}} (classic, oblique, bold and bold oblique as well) fonts
However, as we encountered more errors than in the original paper, we also tried to use the \href{https://fonts.google.com/specimen/Inconsolata}{\textit{Incosolata}} (regular and bold) fonts distributed by Google, which gave far better results.

\begin{figure}[h]
    \centering
    \includegraphics[width = \linewidth]{data/input_freemono.png}
    \includegraphics[width = \linewidth]{data/input_inconsolata.png}
    \caption{Images example of a sequence using 2 different set of fonts. The first image is computed using \textit{FreeMono}, the second is using \textit{Inconsolata}.}
    \label{fig:input_imgs}
\end{figure}

\subsubsection{Noise}

Pascanu and Jaeger didn't specify in the original the probability of the salt-and-pepper noise. Thus, we tried a salt-and-pepper noise of amplitude 0.1, and probability 1. \\

\subsubsection{Warm up in Principal Component Analysis}

We have observed that for each memory state the first few points do not necessarily gather with their cluster. Thus, we removed the first 100 time steps from the display but not to perform the PCA.

\section{Results}
\label{s:result}

\subsection{Network}
According to the former article, the experiment has been run over 30 runs for the network. 
We assumed that each time, it uses different randomly generated training and testing sequences, and reservoir, input and feedback weights. Those 30 runs are then used to compute mean and standard deviation of different performance measures. In the following Tables we used the color light green for the \textit{FreeMono} fonts (first row on result tables), dark green for \textit{Inconsolata} (second row), and black for their former results (third row). \\
\\
First, as they did in the original paper we inspect the WM-units performance.
We dissociate the errors made by the WM-units into false negative errors (when the network did not recognize a curly bracket as such) and false positive errors (when the network detects the character as a curly bracket while it is not).
These errors are counted, and then transformed into three different percentages, depending on the number of curly brackets in the sequence, the number of characters, or the number of time steps (i.e. length of the input image in pixels). The \textbf{Table \ref{tab:errors_wm}} shows the results obtained. \\
\\

\begin{table}[!ht]
    \centering
    \begin{tabular}{|C{23mm}|C{20mm}|C{22mm}|C{24mm}|C{24mm}|}
        \hline
        Type of error & Number of errors & Percentage of curly brackets ($\%$) & Percentage of characters ($\%$) & Percentage of time steps ($\%$) \\
        \hline
        \multirow{3}{*}{false negatives} & \textcolor{mygreen}{$54.7 \pm 6.6$} & \textcolor{mygreen}{$3.01 \pm 0.35$} & \textcolor{mygreen}{$0.16 \pm 0.020$} & \textcolor{mygreen}{$0.022 \pm 0.003$} \\
        & \textcolor{mygreen2}{$0.2 \pm 0.4$} & \textcolor{mygreen2}{$0.01 \pm 0.02$} & \textcolor{mygreen2}{$0 \pm 0.001$} & \textcolor{mygreen2}{$0 \pm 0$} \\
        & $7.2 \pm 6.5$ & $0.34 \pm 0.30$ & $0.02 \pm 0.018$ & $0.003 \pm 0.002$ \\ \hline
       
        \multirow{3}{*}{false positives} & \textcolor{mygreen}{$1252.6 \pm 48.5$} & \textcolor{mygreen}{$68.97 \pm 2.44$} & \textcolor{mygreen}{$3.77 \pm 0.148$} & \textcolor{mygreen}{$0.511 \pm 0.020$} \\
        & \textcolor{mygreen2}{$97.2 \pm 34.5$} & \textcolor{mygreen2}{$5.37 \pm 1.89$} & \textcolor{mygreen2}{$0.29 \pm 0.104$} & \textcolor{mygreen2}{$0.040 \pm 0.014$} \\
        & $59.8 \pm 21.6$ & $2.84 \pm 1.02$ & $0.17 \pm 0.061$ & $0.024 \pm 0.008$ \\ \hline
        
        \multirow{3}{*}{total} & \textcolor{mygreen}{$1307.3 \pm 49.3$} & \textcolor{mygreen}{$71.98 \pm 2.44$} & \textcolor{mygreen}{$3.94 \pm 0.151$} &  \textcolor{mygreen}{$0.534 \pm 0.020$} \\
        & \textcolor{mygreen2}{$97.4 \pm 34.6$} & \textcolor{mygreen2}{$5.37 \pm 1.89$} & \textcolor{mygreen2}{$0.29 \pm 0.104$} & \textcolor{mygreen2}{$0.040 \pm 0.014$} \\
        & $67.0 \pm 22.9$ & $3.18 \pm 1.09$ & $0.19 \pm 0.065$ & $0.027 \pm 0.009$ \\ \hline

    \end{tabular}
    \caption{Number of erroneous WM states obtained by the ESN, averaged over 30 runs. Colour light green is used for results using \textit{FreeMono} as font, dark green for \textit{Inconsolata}, and black for the original results.}
    \label{tab:errors_wm}
\end{table}

Then we looked at the characters falsely identified as curly brackets. The \textbf{Table \ref{tab:false_positives}} shows how the characters highlighted in the original experiment are falsely identified as curly brackets.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|C{15mm}|C{32mm}|C{32mm}|C{32mm}|}
        \hline
        Character & Number of times the character is in testing sequences & Number of times the counter increased & Number of times the counter decreased \\ \hline \hline
        
        \multirow{3}{*}{"("} & \textcolor{mygreen}{$508.7 \pm 23.8$} & \textcolor{mygreen}{$128.07 \pm 14.08$} & \textcolor{mygreen}{$3.8 \pm 2.0$} \\
        & \textcolor{mygreen2}{$514.2 \pm 23.7$} & \textcolor{mygreen2}{$93.40 \pm 34.50$} & \textcolor{mygreen2}{$0 \pm 0$} \\
        & $499.5 \pm 22.3$ & $21.5 \pm 10.1$ & $0 \pm 0$ \\ \hline
        
        \multirow{3}{*}{")"} & \textcolor{mygreen}{$516.7 \pm 19.7$} & \textcolor{mygreen}{$12.10 \pm 4.21$} & \textcolor{mygreen}{$3.9 \pm 1.7$} \\
        & \textcolor{mygreen2}{$516.8 \pm 18.2$} & \textcolor{mygreen2}{$0 \pm 0$} & \textcolor{mygreen2}{$0.3 \pm 0.7$}\\
        & $502.4 \pm 18.6$ & $0 \pm 0$ &  $0.5 \pm 0.2$ \\ \hline
        
        \multirow{3}{*}{"["} & \textcolor{mygreen}{$513.1 \pm 18.8$} & \textcolor{mygreen}{$191.83 \pm 28.71$} & \textcolor{mygreen}{$3.8 \pm 1.6$} \\
        & \textcolor{mygreen2}{$507.9 \pm 15.6$} & \textcolor{mygreen2}{$0 \pm 0$} & \textcolor{mygreen2}{$0 \pm 0$}\\
        & $496.2 \pm 22.8$ & $5.8 \pm 5.1$ & $6.0 \pm 5.4$ \\ \hline
        
        \multirow{3}{*}{"]"} & \textcolor{mygreen}{$509.6 \pm 18.3$} & \textcolor{mygreen}{$9.53 \pm 3.58$} & \textcolor{mygreen}{$3.0 \pm 1.4$}\\
        & \textcolor{mygreen2}{$514.4 \pm 18.3$} & \textcolor{mygreen2}{$0.13 \pm 0.34$} & \textcolor{mygreen2}{$0 \pm 0$}\\
        & $501.3 \pm 15.1$ & $7.2 \pm 6.5$ & $7.2 \pm 6.5$ \\ \hline
        
        \multirow{3}{*}{"@"} & \textcolor{mygreen}{$507.0 \pm 20.8$} & \textcolor{mygreen}{$10.13 \pm 2.84$} & \textcolor{mygreen}{$3.9 \pm 2.1$} \\
        & \textcolor{mygreen2}{$508.8 \pm 22.5$} & \textcolor{mygreen2}{$0 \pm 0$} & \textcolor{mygreen2}{$0 \pm 0$} \\
        & $492.7 \pm 21.3$ & $25.1 \pm 14.1$ & $0.2 \pm 0.1$ \\ \hline
        
        \multirow{3}{*}{other} & \multirow{3}{*}{-} & \textcolor{mygreen}{$656.23 \pm 30.19$} & \textcolor{mygreen}{$226.2 \pm 22.5$} \\
        & & \textcolor{mygreen2}{$3.13 \pm 2.03$} & \textcolor{mygreen2}{$0.2 \pm 0.6$} \\
        & & $0.05 \pm 0.04$ & $0.6 \pm 0.5$ \\ \hline
    \end{tabular}
    \caption{Trigger characters for false positives, averaged over 30 runs. Colour light green is used for results using \textit{FreeMono} as font, dark green for \textit{Inconsolata}, and black for the original results.}
    \label{tab:false_positives}
\end{table}

As they did, we also measured the average absolute value of $\mathbf{W^{mem}}$ weights over the 30 runs, which can be observed in \textbf{Table \ref{tab:wmem}}. \\
\\

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|C{35mm}|}
        \hline
        \begin{tabular}[c]{@{}c@{}}considered weights\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}average absolute value\end{tabular}}\\ \hline \hline
        
        \multirow{3}{*}{input to WM-units weights} &
        \textcolor{mygreen}{$0.3395 \pm 0.2975$} \\ 
        & \textcolor{mygreen2}{$0.2834 \pm 0.2445$} \\ 
        & $0.2327 \pm 0.1813$ \\ \hline
        
        \multirow{3}{*}{reservoir to WM-units weights} &
        \textcolor{mygreen}{$0.0971 \pm 0.0920$} \\ 
        & \textcolor{mygreen2}{$0.0852 \pm 0.0782$} \\ 
        & $0.0667 \pm 0.0591$ \\ \hline
        
        \multirow{3}{*}{WM-units to WM-units weights} &
        \textcolor{mygreen}{$0.9217 \pm 0.6893$} \\ 
        & \textcolor{mygreen2}{$0.7003 \pm 0.5436$} \\ 
        & $0.5825 \pm 0.5627$ \\ \hline
        
    \end{tabular}
    \caption{Average learned output weights of the ESN over 30 runs. Colour light green is used for results using \textit{FreeMono} as font, dark green for \textit{Inconsolata}, and black for the original results.}
    \label{tab:wmem}
\end{table}

As we can see in \textbf{Tables \ref{tab:errors_wm} and \ref{tab:false_positives}}, by using the \textit{FreeMono} font we obtain way more errors in the WM-unit than in the original paper. However by using the \textit{Inconsolata} font we obtain similar results. \\
\\
Finally, we compute the average error rate for the prediction made in the output units. We count only the errors in the middle of the presentation of a character that is not a curly bracket, as the target is defined only during these time steps.
Over the 30 runs, with the \textit{Freemono} font we found an error rate of \textbf{26.02 $\pm$ 0.32\%}, for \textbf{24.83 $\pm$ 0.27\%} in the former article. With the \textit{Inconsolata} font we obtain an error rate of \textbf{22.22 $\pm$ 0.25\%}. These three results are very similar.

\subsection{Attractors}

As in the original paper, for both \textit{Freemono} and \textit{Inconsolata} font we can see clusters of points in the first two principal components(see dark projection in \textbf{Figure \ref{fig:PCA}}). Each of this cluster seems to be an attractor (as they defined in the original paper) associated to a memory state. It is stable against the characters that are not curly brackets, and the curly brackets makes it move from one to the other.

\begin{figure}[h]
    \centering
    \includegraphics[width = .45\linewidth]{data/FreeMono_PCA.png}
    \includegraphics[width = .45\linewidth]{data/Inconsolata_PCA.png}
    \caption{Principal Component Analysis results. Each coloured column represents the several reservoir states for each memory states in the form of attractors, with \textit{FreeMono} results on the left, and \textit{Inconsolata} on the right. For each memory states we do not display the first 100 points.}
    \label{fig:PCA}
\end{figure}

\section{Conclusion}

In the end, we consider that we have been able to successfully replicate the result obtained by Pascanu and Jaeger in \textit{A Neurodynamical Model for Working Memory}.
%With the same ESN-based model (i.e. mainly composed of random connections)
Very few information were missing in the original paper, such as the exact font they used, a more precise definition of the noise they added or the ASCII characters they used.
